layers,activation,batch_size,mae,mae_lower,mae_upper,rmse,rmse_lower,rmse_upper
64,prelu,32,0.23241596691714148,0.209338007114204,0.25464966797476324,0.3230546588867277,0.286751725613888,0.35584282269839607
