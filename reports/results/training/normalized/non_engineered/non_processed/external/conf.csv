layers,activation,batch_size,mae,mae_lower,mae_upper,rmse,rmse_lower,rmse_upper
16,prelu,64,0.2329185185174051,0.2095084318151617,0.25651412710180443,0.3240704691159217,0.2876107335523069,0.35884604282316984
