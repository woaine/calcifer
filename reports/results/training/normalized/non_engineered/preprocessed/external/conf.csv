layers,activation,batch_size,mae,mae_lower,mae_upper,rmse,rmse_lower,rmse_upper
64,prelu,32,0.23829137101958084,0.21571563247487516,0.2586202273308477,0.3288197217199554,0.2946625511245946,0.3634670227192933
