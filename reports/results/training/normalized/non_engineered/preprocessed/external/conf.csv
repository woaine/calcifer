layers,activation,batch_size,mae,mae_lower,mae_upper,rmse,rmse_lower,rmse_upper
32,prelu,32,0.23783556588088428,0.2154933522864236,0.25772286876243894,0.3280584631384564,0.2948102941812231,0.3622164515276438
