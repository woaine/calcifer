layers,activation,batch_size,mae,mae_lower,mae_upper,rmse,rmse_lower,rmse_upper
64,prelu,32,0.2612642114988624,0.24983884519154143,0.2733150969901586,0.36737535024667417,0.34699791798494195,0.3885154934623322
