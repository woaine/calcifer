layers,activation,batch_size,mae,mae_lower,mae_upper,rmse,rmse_lower,rmse_upper
64,prelu,32,0.261671818191784,0.25032954961620957,0.27360885612375563,0.3678132300788502,0.34786792171812764,0.38870406190886875
