layers,activation,batch_size,mae,mae_lower,mae_upper,rmse,rmse_lower,rmse_upper
64,prelu,64,0.26118313205357085,0.24982566963168723,0.2729737955709815,0.3670148492069804,0.3465028117722315,0.38769678787023915
