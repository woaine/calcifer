layers,activation,batch_size,mae,mae_lower,mae_upper,rmse,rmse_lower,rmse_upper
64,prelu,32,0.24071271872218672,0.2183236868653119,0.26090035696874714,0.3311911570761884,0.297186713818915,0.36633589279682965
