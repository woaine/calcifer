layers,activation,batch_size,mae,mae_lower,mae_upper,rmse,rmse_lower,rmse_upper
64,prelu,32,0.23550386475812055,0.2131582845490555,0.2586029886612165,0.3262502046543922,0.2903265108204883,0.3591197770125796
