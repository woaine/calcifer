layers,activation,batch_size,mae,mae_lower,mae_upper,rmse,rmse_lower,rmse_upper
64,prelu,32,0.26266600569813886,0.25171243478653776,0.2736975198616944,0.3673877413508998,0.34894217629783014,0.38442768369859776
